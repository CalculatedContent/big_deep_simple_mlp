#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Quotes eld
\end_inset

What optimization problem is deep learning solving?
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
We compare unsupervised deep beleief nets (RBMs, DBNs, etc) and supervised
 deep nets (MLPs, CNNs, RNNs, etc) 
\end_layout

\begin_layout Standard
Unsupervised DBNs have been described by Bengio (in his new book), as well
 as by Ng, Larochelle, etc.
 These nets are specififed using a statistical mechanics formalism, including
 specifying the Hamiltonian, Free Energy, and Partition funciton.
 Infernce is achieved by monte carlo sampling techniques of various forms.
 
\end_layout

\begin_layout Standard
Supervised Deep Nets are supported by modern frameworks like Google Tensorflow.
 They are specified using only an Energy function, and inference uses Stochastic
 Gradient Descent.
 Curiosuly, the Deep Net Energy functions do not resemble traditional Hopfield
 Nets or Ising Models, but, rather, resemble the form of the log conditional
 probabilities arising in Unsupervised DBMs like RBMs.
 
\end_layout

\begin_layout Standard
This begs the question...do Supervised Deep Nets, training with SGD, effectively
 (implicitly) run a monte carlo sampling ?
\end_layout

\begin_layout Standard
Consider a typical MLP, with 1 output vector (o), 2 hidden laters (h,k),
 and visbile units (v)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{o}=\mathbf{W^{ok}k}+\mathbf{a^{T}k}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{k}=ReLu(\mathbf{W^{kh}h}+\mathbf{b^{T}h})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{h}=ReLu(\mathbf{W^{hv}v}+\mathbf{c^{T}v})
\]

\end_inset


\end_layout

\begin_layout Standard
The outputs are typically converted to probabilities using a softmax transform,
 yielding a probabilistic prediciton for a class label 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{c}^{pred}=Softmax(\mathbf{o)_{c}}
\]

\end_inset


\end_layout

\begin_layout Standard
and the MLP minimizes the cross entropy between the true and predicted class
 labels
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\min\sum_{c}y_{c}^{true}ln(y_{c}^{pred})
\]

\end_inset


\end_layout

\begin_layout Standard
(although in TensorFlow, the crossentropy and softmax are usually combined
 into 1 single optimization function)
\end_layout

\begin_layout Standard
see https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_
NeuralNetworks/multilayer_perceptron.ipynb 
\end_layout

\begin_layout Standard
Now consider that the activation function, the Rectified Linear Unit (ReLu)
 is approximations the log sigmoid function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
ReLu(x)\sim ln(1+e^{x})
\]

\end_inset


\end_layout

\begin_layout Standard
and is applied pointwise to a vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 
\end_layout

\begin_layout Standard
Note: when we take a sum of ReLus, we get a product of sigmoids
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sum_{j}ReLu(x_{j})=\sum_{j}ln(1+e^{x_{j}})=ln(\prod_{j}(1+e^{x_{j}}))
\]

\end_inset


\end_layout

\begin_layout Standard
finally, WLOG, let the bias terms a, b,c = 0
\end_layout

\begin_layout Standard
We can now write 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sum_{j}\mathbf{k}_{j}=\sum_{j}Relu(\mathbf{(W^{kh}h})_{j})=\sum_{j}ln(1+exp((\mathbf{W^{kh}h})_{j})=ln(\prod_{j}(1+exp((\mathbf{W^{kh}h})))
\]

\end_inset


\end_layout

\begin_layout Standard
where j sums over the nodes in the Hidden layer k
\end_layout

\begin_layout Standard
Let us now assume that each node in the neural network can activated or
 not, and, therefore, exists in a superposition of on/off states.
 We therefore assign a state vector 
\begin_inset Formula $\mathbf{c}_{n}$
\end_inset

 to each node (n)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{c}_{n}=[0,1]
\]

\end_inset


\end_layout

\begin_layout Standard
This gives something like
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{k}=ln(\prod_{j}(exp((\mathbf{cW^{kh}h})_{j}))
\]

\end_inset


\end_layout

\begin_layout Standard
so we have now a way to get at a free energy for each layer...this is probably
 not correct but the intent is here...more to come
\end_layout

\begin_layout Standard
Because we are running SGD on a training set, we may write o,k,h as a function
 of x explicitly
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{k}(\mathbf{x})=ln(\prod_{j}(1+exp((\mathbf{W^{kh}h(x)})_{j}))
\]

\end_inset


\series bold
using some of this
\end_layout

\begin_layout Standard
The SGD trained nets propagate errors using back-prop.
 This leads, seemingly, to walking an energy landscape.
 In DBNs, and in graphical methods generally, we propagate normalized probabilit
ies.
 Lets take a look at the normalization in an SGD trained Deep Net (SGD-DN)
 , starting with the final softmax 
\end_layout

\begin_layout Standard
On every batch 
\begin_inset Formula $\mathcal{B}$
\end_inset

 of training examples 
\begin_inset Formula $\mathbf{x}$
\end_inset

, we compute the normalization, sampled over the batch, along each output
 node 
\begin_inset Formula $o_{c}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Z(\mathbf{o},\mathcal{B})=\sum_{c}\sum_{\mathbf{x}\in\mathcal{B}}\exp[o_{c}(\mathbf{x})]
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
o_{c}(\mathbf{x})=\mathbf{W}_{c}^{ok}\mathbf{k(x)}
\]

\end_inset


\end_layout

\begin_layout Standard
We may be able to simplify k(x)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
k(\mathbf{x})=ln(1+\exp(\mathbf{W}^{kh}\mathbf{h}(\mathbf{x})))
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
k(\mathbf{x})=ln(1+\exp(\sum_{j=1}^{N_{h}}W^{kh}h(\mathbf{\mathbf{x}})))
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
k(\mathbf{x})=ln(1+\prod_{j=1}^{N_{h}}\exp(W_{j}^{kh}h_{j}(\mathbf{\mathbf{x}})))
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $o_{c}$
\end_inset

 has units of energy, and it would be nice if we could play some 'tricks'
 to get rid of the 1+product by some reasonable approximation
\end_layout

\begin_layout Standard
such as introducing the c=[0,1] vector
\end_layout

\begin_layout Standard
that is, if we could get an expression like
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
k(\mathbf{x})\thickapprox ln(\prod_{j=1}^{N_{_{h}}}\exp(X_{cj}h_{j}(\mathbf{\mathbf{x}})))
\]

\end_inset


\end_layout

\begin_layout Standard
so that we can write each node as a sum of terms
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
k(\mathbf{x})\thickapprox\sum_{j=1}^{N_{k}}X_{cj}h_{j}(\mathbf{\mathbf{x}})
\]

\end_inset


\end_layout

\begin_layout Standard
or some reasonable form so we can continue to expand k and h in some linear
 way, at least as a first approximation
\end_layout

\begin_layout Standard
this may be too crude to work with ...
\end_layout

\begin_layout Standard

\series bold
Free Energy per Layer
\end_layout

\begin_layout Standard
I have been trying to get an expression for the free energy per layer
\end_layout

\begin_layout Standard
That is, for k, h, and v, we want to define / construct
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{F}^{k}=-ln\sum_{k}e^{-\mathcal{H}(\mathbf{k(x)})}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathcal{H}(\mathbf{k(x))}$
\end_inset

 is the renormalized Hamiltonian / Energy function for the K hidden layer
\end_layout

\begin_layout Standard
although in SGD I assume we are also sampling over the x in the Data set
 / batch, which I dont show here
\end_layout

\begin_layout Standard
likewise, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{F}^{h}=-ln\sum_{h}e^{-\mathcal{H}(\mathbf{h(x)})}
\]

\end_inset


\end_layout

\begin_layout Standard
and the Free Energy of the visibile units is defined by an energy function
 
\begin_inset Formula $\mathcal{H}(\mathbf{v(x)})$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{F}^{v}=-ln\sum_{v}e^{-\mathcal{H}(\mathbf{v(x)})}
\]

\end_inset


\end_layout

\begin_layout Standard
During training, we want these Free Energies to be similar, so that 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\varDelta\mathcal{F=}\mathcal{F}^{k}-\mathcal{F}^{h}=\mathcal{F}^{h}-\mathcal{F}^{v}=0
\]

\end_inset


\end_layout

\begin_layout Standard
We note that the total partition function for , say , 1 Hidden layer, if
 we could define it, would be
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Z(h,v)=\sum_{h}\sum_{v}e^{-E(h,v)}
\]

\end_inset


\end_layout

\begin_layout Standard
as we do in say an RBM
\end_layout

\begin_layout Standard
so if we can write
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E(h,v)=H(v)+V(v,h)
\]

\end_inset


\end_layout

\begin_layout Standard
giving
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Z=\sum_{v}e^{-\mathcal{H}(v)}=\sum_{h,v}e^{-\mathcal{H}(v)}e^{-\mathcal{H}(v,h)}
\]

\end_inset


\end_layout

\begin_layout Standard
which gives the 
\series bold
intermediate normalization constraint
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sum_{h}e^{-V(h,v)}=1
\]

\end_inset


\end_layout

\begin_layout Standard
which I am hoping can be used as a constraint to show how the SGD training
 'effectively' propagates something like a normalized , perhaps conditional
 probability through the net
\end_layout

\end_body
\end_document
