{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-04-20T21:52:04.548207",
     "start_time": "2016-04-20T21:52:04.544771"
    }
   },
   "source": [
    "## 5 Layer MLP w/InfiMNIST 10k\n",
    "\n",
    "10,000 epochs of RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:42.303339",
     "start_time": "2016-05-04T22:21:41.057014"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:42.307432",
     "start_time": "2016-05-04T22:21:42.304414"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run infimnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:42.607897",
     "start_time": "2016-05-04T22:21:42.605546"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "batch_size = 125\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:43.245224",
     "start_time": "2016-05-04T22:21:43.241142"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 2000 # 1st layer num features\n",
    "n_hidden_2 = 1500 # 1st layer num features\n",
    "n_hidden_3 = 1000 # 1st layer num features\n",
    "n_hidden_4 = 500 # 2nd layer num features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "std_0 = 1.0/np.sqrt(n_input)\n",
    "std_h1 = 1.0/np.sqrt(n_hidden_1)\n",
    "std_h2 = 1.0/np.sqrt(n_hidden_2)\n",
    "std_h3 = 1.0/np.sqrt(n_hidden_3)\n",
    "std_h4 = 1.0/np.sqrt(n_hidden_4)\n",
    "\n",
    "logfile = \"5-layer-mlp-infimnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:43.727964",
     "start_time": "2016-05-04T22:21:43.721861"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "xt = tf.placeholder(\"float\", [None, n_input])\n",
    "yt = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:44.308399",
     "start_time": "2016-05-04T22:21:44.305404"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(_X, _weights, _biases):\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) \n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, _weights['h2']), _biases['b2'])) \n",
    "    layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, _weights['h3']), _biases['b3'])) \n",
    "    layer_4 = tf.nn.relu(tf.add(tf.matmul(layer_3, _weights['h4']), _biases['b4'])) \n",
    "\n",
    "    return tf.matmul(layer_4, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:44.956518",
     "start_time": "2016-05-04T22:21:44.921159"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=std_0)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=std_h1)),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3], stddev=std_h2)),\n",
    "    'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4], stddev=std_h3)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_4, n_classes], stddev=std_h4))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1], stddev=0.1)),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2], stddev=0.01)),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3], stddev=0.01)),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden_4], stddev=0.01)),\n",
    "\n",
    "    'out': tf.Variable(tf.random_normal([n_classes], stddev=0.001))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:45.346716",
     "start_time": "2016-05-04T22:21:45.336865"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mlp = multilayer_perceptron(x, weights, biases )\n",
    "mlp_test = multilayer_perceptron(xt, weights, biases )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:45.858972",
     "start_time": "2016-05-04T22:21:45.850052"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost =  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(mlp, y)) \n",
    "gs = tf.get_variable(\"global_step\",[],trainable=False,initializer=tf.constant_initializer(0))\n",
    "lr = tf.constant(learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:46.354400",
     "start_time": "2016-05-04T22:21:46.217038"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_op = tf.contrib.layers.optimize_loss(cost, global_step=gs, learning_rate=lr,optimizer=\"RMSProp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:46.701651",
     "start_time": "2016-05-04T22:21:46.681418"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ScalarSummary_2:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with tf.name_scope(\"training accuracy\"):\n",
    "pred = tf.equal(tf.argmax(mlp, 1), tf.argmax(y, 1)) # Count correct predictions\n",
    "train_acc_op = tf.reduce_mean(tf.cast(pred, \"float\"))  # Cast boolean to float to average\n",
    "tf.scalar_summary(\"training accuracy\", train_acc_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:47.060583",
     "start_time": "2016-05-04T22:21:47.049444"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ScalarSummary_3:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = tf.equal(tf.argmax(mlp_test, 1), tf.argmax(yt, 1)) # Count correct predictions\n",
    "test_acc_op = tf.reduce_mean(tf.cast(test_pred, \"float\"))  # Cast boolean to float to average\n",
    "tf.scalar_summary(\"test 0 accuracy\", test_acc_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:48.210893",
     "start_time": "2016-05-04T22:21:48.208976"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infiminst = InfiMNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:21:51.522596",
     "start_time": "2016-05-04T22:21:51.288843"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2-layer-mlp-infimnist\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./logs/{logfile}\n",
    "!ls logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T21:57:37.534602",
     "start_time": "2016-05-04T21:57:37.532739"
    },
    "collapsed": true
   },
   "source": [
    "### Original MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-05-04T22:22:16.521068",
     "start_time": "2016-05-04T22:22:12.792793"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/gzip.py:275: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  chunk = self.extrabuf[offset: offset + size]\n",
      "input_data.py:35: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  data = data.reshape(num_images, rows, cols, 1)\n"
     ]
    }
   ],
   "source": [
    "infiminst.next_epoch()\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True)\n",
    "trX_0, trY_0 = mnist.train.images, mnist.train.labels\n",
    "teX_0, teY_0 = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2016-05-05T05:22:36.526Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.91018182, 0.91289997)\n",
      "(1, 0.96670908, 0.9677)\n",
      "(2, 0.96321815, 0.96429998)\n",
      "(3, 0.95221817, 0.94630003)\n",
      "(4, 0.9806, 0.97530001)\n",
      "(5, 0.97541821, 0.96850002)\n",
      "(6, 0.96899998, 0.9648)\n",
      "(7, 0.97820002, 0.9756)\n",
      "(8, 0.97714543, 0.9734)\n",
      "(9, 0.98418182, 0.98150003)\n",
      "(10, 0.98505455, 0.98259997)\n",
      "(11, 0.98305452, 0.97759998)\n",
      "(12, 0.98147273, 0.97670001)\n",
      "(13, 0.97950912, 0.97689998)\n",
      "(14, 0.97658181, 0.97390002)\n",
      "(15, 0.98889089, 0.98509997)\n",
      "(16, 0.98450911, 0.98089999)\n",
      "(17, 0.9743818, 0.96509999)\n",
      "(18, 0.98580003, 0.98119998)\n",
      "(19, 0.9732182, 0.96460003)\n",
      "(20, 0.98647273, 0.98360002)\n",
      "(21, 0.99145454, 0.98610002)\n",
      "(22, 0.99094546, 0.98400003)\n",
      "(23, 0.98680001, 0.98220003)\n",
      "(24, 0.9867273, 0.97890002)\n",
      "(25, 0.98294544, 0.97780001)\n",
      "(26, 0.98823637, 0.98189998)\n",
      "(27, 0.99163634, 0.98659998)\n",
      "(28, 0.99072725, 0.98519999)\n",
      "(29, 0.99187273, 0.98549998)\n",
      "(30, 0.99003637, 0.98280001)\n",
      "(31, 0.98640001, 0.98000002)\n",
      "(32, 0.98321819, 0.97960001)\n",
      "(33, 0.99089092, 0.98439997)\n",
      "(34, 0.99216366, 0.98379999)\n",
      "(35, 0.98870909, 0.98540002)\n",
      "(36, 0.99049091, 0.98549998)\n",
      "(37, 0.98610908, 0.97920001)\n",
      "(38, 0.98396361, 0.977)\n",
      "(39, 0.98881817, 0.98220003)\n",
      "(40, 0.98561817, 0.97909999)\n",
      "(41, 0.98970908, 0.98250002)\n",
      "(42, 0.99132729, 0.98430002)\n",
      "(43, 0.98649091, 0.97930002)\n",
      "(44, 0.99323636, 0.98519999)\n",
      "(45, 0.98994547, 0.9813)\n",
      "(46, 0.9836182, 0.97030002)\n",
      "(47, 0.99247271, 0.98659998)\n",
      "(48, 0.98663634, 0.97960001)\n",
      "(49, 0.99269092, 0.98439997)\n",
      "(50, 0.98972726, 0.98320001)\n",
      "(51, 0.99080002, 0.98430002)\n",
      "(52, 0.98918182, 0.98390001)\n",
      "(53, 0.98661816, 0.97579998)\n",
      "(54, 0.98772728, 0.98299998)\n",
      "(55, 0.98285455, 0.97109997)\n",
      "(56, 0.99345452, 0.98549998)\n",
      "(57, 0.98178184, 0.9734)\n",
      "(58, 0.99118179, 0.9831)\n",
      "(59, 0.98912728, 0.98320001)\n",
      "(60, 0.99052727, 0.98250002)\n",
      "(61, 0.99076366, 0.9835)\n",
      "(62, 0.99109089, 0.98199999)\n",
      "(63, 0.98843634, 0.98250002)\n",
      "(64, 0.99394548, 0.98699999)\n",
      "(65, 0.98979998, 0.98180002)\n",
      "(66, 0.99147272, 0.98479998)\n",
      "(67, 0.99089092, 0.9799)\n",
      "(68, 0.99069089, 0.9842)\n",
      "(69, 0.99143636, 0.98320001)\n",
      "(70, 0.99325454, 0.98400003)\n",
      "(71, 0.97423637, 0.9558)\n",
      "(72, 0.99116361, 0.98369998)\n",
      "(73, 0.99223638, 0.98430002)\n",
      "(74, 0.99069089, 0.98070002)\n",
      "(75, 0.98869091, 0.97909999)\n",
      "(76, 0.98563635, 0.9795)\n",
      "(77, 0.99172729, 0.98119998)\n",
      "(78, 0.99101818, 0.9835)\n",
      "(79, 0.98674548, 0.98110002)\n",
      "(80, 0.99183637, 0.98390001)\n",
      "(81, 0.98554546, 0.98079997)\n",
      "(82, 0.9897818, 0.98159999)\n",
      "(83, 0.98729092, 0.9788)\n",
      "(84, 0.99101818, 0.98369998)\n",
      "(85, 0.98974544, 0.98140001)\n",
      "(86, 0.98650908, 0.97869998)\n",
      "(87, 0.98787272, 0.98269999)\n",
      "(88, 0.99029088, 0.9813)\n",
      "(89, 0.98198181, 0.97689998)\n",
      "(90, 0.99276364, 0.98619998)\n",
      "(91, 0.98952729, 0.9831)\n",
      "(92, 0.98943639, 0.98229998)\n",
      "(93, 0.99087274, 0.98320001)\n",
      "(94, 0.9891091, 0.98269999)\n",
      "(95, 0.98887271, 0.9824)\n",
      "(96, 0.98512727, 0.97509998)\n",
      "(97, 0.98703635, 0.97860003)\n",
      "(98, 0.99159998, 0.98369998)\n",
      "(99, 0.99199998, 0.98400003)\n",
      "(100, 0.99130911, 0.98360002)\n",
      "(101, 0.98758179, 0.9817)\n",
      "(102, 0.98979998, 0.98049998)\n",
      "(103, 0.99003637, 0.98070002)\n",
      "(104, 0.98085457, 0.97799999)\n",
      "(105, 0.98000002, 0.97280002)\n",
      "(106, 0.98996365, 0.98379999)\n",
      "(107, 0.98045456, 0.97490001)\n",
      "(108, 0.99007273, 0.98140001)\n",
      "(109, 0.98458183, 0.9774)\n",
      "(110, 0.98781818, 0.9806)\n",
      "(111, 0.99038184, 0.9813)\n",
      "(112, 0.99194545, 0.9831)\n",
      "(113, 0.99269092, 0.98559999)\n",
      "(114, 0.99254543, 0.98479998)\n",
      "(115, 0.99067271, 0.98409998)\n",
      "(116, 0.99138182, 0.98159999)\n",
      "(117, 0.9909091, 0.9835)\n",
      "(118, 0.98950911, 0.98199999)\n",
      "(119, 0.98876363, 0.98079997)\n",
      "(120, 0.99185455, 0.98360002)\n",
      "(121, 0.98736364, 0.97979999)\n",
      "(122, 0.98372728, 0.97860003)\n",
      "(123, 0.98623633, 0.97899997)\n",
      "(124, 0.98518181, 0.97759998)\n",
      "(125, 0.98803633, 0.98259997)\n",
      "(126, 0.98907274, 0.98299998)\n",
      "(127, 0.9903273, 0.98339999)\n",
      "(128, 0.99374545, 0.98680001)\n",
      "(129, 0.99230909, 0.98509997)\n",
      "(130, 0.98981816, 0.9817)\n",
      "(131, 0.99312729, 0.98409998)\n",
      "(132, 0.99063635, 0.9824)\n",
      "(133, 0.9927091, 0.986)\n",
      "(134, 0.98949093, 0.98159999)\n",
      "(135, 0.99267274, 0.98470002)\n",
      "(136, 0.98610908, 0.97850001)\n",
      "(137, 0.98956364, 0.98220003)\n",
      "(138, 0.98952729, 0.98290002)\n",
      "(139, 0.98710907, 0.97960001)\n",
      "(140, 0.98694545, 0.97909999)\n",
      "(141, 0.99141818, 0.98360002)\n",
      "(142, 0.99169093, 0.98460001)\n",
      "(143, 0.98763639, 0.98119998)\n",
      "(144, 0.9873091, 0.98070002)\n",
      "(145, 0.98950911, 0.98299998)\n",
      "(146, 0.98772728, 0.97850001)\n",
      "(147, 0.98329091, 0.97839999)\n",
      "(148, 0.98796362, 0.98199999)\n",
      "(149, 0.98989093, 0.98250002)\n",
      "(150, 0.98334545, 0.97539997)\n",
      "(151, 0.97785455, 0.97210002)\n",
      "(152, 0.9903273, 0.98369998)\n",
      "(153, 0.98690909, 0.97890002)\n",
      "(154, 0.9816727, 0.9763)\n",
      "(155, 0.98232728, 0.9774)\n",
      "(156, 0.9872182, 0.98040003)\n",
      "(157, 0.98801816, 0.97979999)\n",
      "(158, 0.98318183, 0.9788)\n",
      "(159, 0.98554546, 0.98119998)\n",
      "(160, 0.98589092, 0.97710001)\n",
      "(161, 0.98956364, 0.9824)\n",
      "(162, 0.98341817, 0.97659999)\n",
      "(163, 0.98438179, 0.97860003)\n",
      "(164, 0.98205453, 0.97530001)\n",
      "(165, 0.97574544, 0.9702)\n",
      "(166, 0.98985457, 0.98339999)\n",
      "(167, 0.99012727, 0.98360002)\n",
      "(168, 0.98574543, 0.97680002)\n",
      "(169, 0.96410906, 0.95719999)\n",
      "(170, 0.97598183, 0.96649998)\n",
      "(171, 0.97361821, 0.96579999)\n",
      "(172, 0.98758179, 0.98089999)\n",
      "(173, 0.98761821, 0.98199999)\n",
      "(174, 0.96734548, 0.96219999)\n",
      "(175, 0.98049092, 0.97310001)\n",
      "(176, 0.97774547, 0.97299999)\n",
      "(177, 0.98645455, 0.98009998)\n",
      "(178, 0.97852725, 0.97030002)\n",
      "(179, 0.98305452, 0.97640002)\n",
      "(180, 0.98472726, 0.97790003)\n",
      "(181, 0.98025453, 0.9727)\n",
      "(182, 0.98439997, 0.97839999)\n",
      "(183, 0.95143634, 0.9418)\n",
      "(184, 0.96632725, 0.96039999)\n",
      "(185, 0.98189092, 0.97250003)\n",
      "(186, 0.98074543, 0.972)\n",
      "(187, 0.98045456, 0.97420001)\n",
      "(188, 0.98218185, 0.97289997)\n",
      "(189, 0.97005457, 0.9655)\n",
      "(190, 0.97719997, 0.97250003)\n",
      "(191, 0.97836363, 0.9702)\n",
      "(192, 0.97374547, 0.97060001)\n",
      "(193, 0.97887272, 0.97070003)\n",
      "(194, 0.98301816, 0.97589999)\n",
      "(195, 0.97998184, 0.97280002)\n",
      "(196, 0.97527272, 0.97460002)\n",
      "(197, 0.95943636, 0.95700002)\n",
      "(198, 0.97203636, 0.96649998)\n",
      "(199, 0.97243637, 0.9666)\n",
      "(200, 0.96834546, 0.96450001)\n",
      "(201, 0.97820002, 0.97420001)\n",
      "(202, 0.97905457, 0.97430003)\n",
      "(203, 0.98098183, 0.97589999)\n",
      "(204, 0.94863635, 0.93889999)\n",
      "(205, 0.9697091, 0.9648)\n",
      "(206, 0.9703818, 0.96719998)\n",
      "(207, 0.97001821, 0.96579999)\n",
      "(208, 0.95672727, 0.95499998)\n",
      "(209, 0.98461819, 0.97939998)\n",
      "(210, 0.96843636, 0.96460003)\n",
      "(211, 0.97174543, 0.97039998)\n",
      "(212, 0.9666909, 0.96490002)\n",
      "(213, 0.9532727, 0.95410001)\n",
      "(214, 0.97425455, 0.97329998)\n",
      "(215, 0.95036364, 0.94520003)\n",
      "(216, 0.97121817, 0.96600002)\n",
      "(217, 0.95423639, 0.94980001)\n",
      "(218, 0.96963638, 0.96380001)\n",
      "(219, 0.96598184, 0.9605)\n",
      "(220, 0.97110909, 0.96969998)\n",
      "(221, 0.93559998, 0.93419999)\n",
      "(222, 0.96190912, 0.958)\n",
      "(223, 0.96527272, 0.96329999)\n",
      "(224, 0.96350908, 0.96069998)\n",
      "(225, 0.90154546, 0.90600002)\n",
      "(226, 0.93832725, 0.94139999)\n",
      "(227, 0.93949091, 0.94090003)\n",
      "(228, 0.95612729, 0.95370001)\n",
      "(229, 0.91787273, 0.92320001)\n",
      "(230, 0.88994545, 0.89579999)\n",
      "(231, 0.95272726, 0.95090002)\n",
      "(232, 0.95112729, 0.94760001)\n",
      "(233, 0.96007276, 0.95789999)\n",
      "(234, 0.93930912, 0.93690002)\n",
      "(235, 0.93267274, 0.92879999)\n",
      "(236, 0.9041273, 0.90799999)\n",
      "(237, 0.94330907, 0.94559997)\n",
      "(238, 0.95847273, 0.958)\n",
      "(239, 0.87958181, 0.87269998)\n",
      "(240, 0.92445457, 0.9271)\n",
      "(241, 0.94836366, 0.9522)\n",
      "(242, 0.92538184, 0.926)\n",
      "(243, 0.92214543, 0.92540002)\n",
      "(244, 0.92583638, 0.92900002)\n",
      "(245, 0.91376364, 0.91729999)\n",
      "(246, 0.91834545, 0.92339998)\n",
      "(247, 0.94621819, 0.94770002)\n",
      "(248, 0.92803639, 0.92869997)\n",
      "(249, 0.91403639, 0.92570001)\n",
      "(250, 0.87761819, 0.89050001)\n",
      "(251, 0.91301817, 0.91250002)\n",
      "(252, 0.89281815, 0.90859997)\n",
      "(253, 0.89494544, 0.9034)\n",
      "(254, 0.90996361, 0.91509998)\n",
      "(255, 0.92963636, 0.93199998)\n",
      "(256, 0.92190909, 0.91649997)\n",
      "(257, 0.90261817, 0.90859997)\n",
      "(258, 0.93514544, 0.93879998)\n",
      "(259, 0.87681818, 0.88709998)\n",
      "(260, 0.89229089, 0.90100002)\n",
      "(261, 0.91912729, 0.9296)\n",
      "(262, 0.94563639, 0.9515)\n",
      "(263, 0.95830911, 0.95859998)\n",
      "(264, 0.95527273, 0.95920002)\n",
      "(265, 0.90107274, 0.9077)\n",
      "(266, 0.94205457, 0.94220001)\n",
      "(267, 0.90579998, 0.9073)\n",
      "(268, 0.88981819, 0.90509999)\n",
      "(269, 0.89232725, 0.90210003)\n",
      "(270, 0.86170912, 0.86440003)\n",
      "(271, 0.91912729, 0.9174)\n",
      "(272, 0.90645456, 0.9181)\n",
      "(273, 0.94487274, 0.94450003)\n",
      "(274, 0.94314545, 0.94749999)\n",
      "(275, 0.94574547, 0.94809997)\n",
      "(276, 0.96889091, 0.97170001)\n",
      "(277, 0.94734544, 0.95020002)\n",
      "(278, 0.9157818, 0.91869998)\n",
      "(279, 0.91718179, 0.91829997)\n",
      "(280, 0.84541816, 0.85829997)\n",
      "(281, 0.89643639, 0.90530002)\n",
      "(282, 0.93005455, 0.93400002)\n",
      "(283, 0.89596361, 0.90249997)\n",
      "(284, 0.89723635, 0.90259999)\n",
      "(285, 0.77887273, 0.79500002)\n",
      "(286, 0.89403635, 0.90700001)\n",
      "(287, 0.85939997, 0.86909997)\n",
      "(288, 0.87523639, 0.89359999)\n",
      "(289, 0.89730906, 0.90979999)\n",
      "(290, 0.93725455, 0.94019997)\n",
      "(291, 0.8792727, 0.89170003)\n",
      "(292, 0.87379998, 0.88440001)\n",
      "(293, 0.89899999, 0.90570003)\n",
      "(294, 0.91809088, 0.9242)\n",
      "(295, 0.84807271, 0.86680001)\n",
      "(296, 0.89072728, 0.89709997)\n",
      "(297, 0.90438181, 0.91659999)\n",
      "(298, 0.90896362, 0.91720003)\n",
      "(299, 0.93374544, 0.94029999)\n",
      "(300, 0.86912727, 0.87690002)\n",
      "(301, 0.65883636, 0.6674)\n",
      "(302, 0.96285456, 0.96319997)\n",
      "(303, 0.86152726, 0.87589997)\n",
      "(304, 0.91305453, 0.92510003)\n",
      "(305, 0.8756727, 0.89429998)\n",
      "(306, 0.92519999, 0.92860001)\n",
      "(307, 0.80632728, 0.82779998)\n",
      "(308, 0.91129088, 0.92559999)\n",
      "(309, 0.884, 0.90310001)\n",
      "(310, 0.81409091, 0.82160002)\n",
      "(311, 0.88814545, 0.90060002)\n",
      "(312, 0.91065454, 0.9206)\n",
      "(313, 0.91625452, 0.92799997)\n",
      "(314, 0.89725453, 0.91049999)\n",
      "(315, 0.89627272, 0.90850002)\n",
      "(316, 0.87249088, 0.88889998)\n",
      "(317, 0.83392727, 0.85479999)\n",
      "(318, 0.89127272, 0.90829998)\n",
      "(319, 0.8665818, 0.87840003)\n",
      "(320, 0.91818184, 0.92460001)\n",
      "(321, 0.90441817, 0.9199)\n",
      "(322, 0.90932727, 0.9192)\n",
      "(323, 0.87563634, 0.89200002)\n",
      "(324, 0.9125818, 0.92739999)\n",
      "(325, 0.84394544, 0.86720002)\n",
      "(326, 0.89069092, 0.90820003)\n",
      "(327, 0.93312728, 0.94480002)\n",
      "(328, 0.87658185, 0.89740002)\n",
      "(329, 0.83683634, 0.86269999)\n",
      "(330, 0.93354547, 0.94529998)\n",
      "(331, 0.88419998, 0.89910001)\n",
      "(332, 0.88018179, 0.90259999)\n",
      "(333, 0.81703639, 0.84369999)\n",
      "(334, 0.81630909, 0.83969998)\n",
      "(335, 0.91860002, 0.9253)\n",
      "(336, 0.90521818, 0.91640002)\n",
      "(337, 0.82123637, 0.8452)\n",
      "(338, 0.81003636, 0.8362)\n",
      "(339, 0.78030908, 0.80989999)\n",
      "(340, 0.91474545, 0.9321)\n",
      "(341, 0.82254547, 0.84969997)\n",
      "(342, 0.91460001, 0.92720002)\n",
      "(343, 0.90483636, 0.92009997)\n",
      "(344, 0.85263634, 0.87110001)\n",
      "(345, 0.88716364, 0.90399998)\n",
      "(346, 0.92830908, 0.93919998)\n",
      "(347, 0.81900001, 0.84570003)\n",
      "(348, 0.89476365, 0.91259998)\n",
      "(349, 0.93152726, 0.9393)\n",
      "(350, 0.84512728, 0.86849999)\n",
      "(351, 0.86945456, 0.87919998)\n",
      "(352, 0.86538184, 0.88330001)\n",
      "(353, 0.88887274, 0.9084)\n",
      "(354, 0.86947274, 0.89060003)\n",
      "(355, 0.87650907, 0.89380002)\n",
      "(356, 0.96885455, 0.9716)\n",
      "(357, 0.97418183, 0.97579998)\n",
      "(358, 0.86778182, 0.88770002)\n",
      "(359, 0.778, 0.81)\n",
      "(360, 0.89632726, 0.91509998)\n",
      "(361, 0.94316363, 0.9526)\n",
      "(362, 0.87225455, 0.8933)\n",
      "(363, 0.84503639, 0.86570001)\n",
      "(364, 0.86781818, 0.88590002)\n",
      "(365, 0.86919999, 0.89459997)\n",
      "(366, 0.84761816, 0.87739998)\n",
      "(367, 0.82847273, 0.86070001)\n",
      "(368, 0.86834544, 0.89200002)\n",
      "(369, 0.88641816, 0.90069997)\n",
      "(370, 0.94603634, 0.95359999)\n",
      "(371, 0.85278183, 0.87620002)\n",
      "(372, 0.85836363, 0.8768)\n",
      "(373, 0.80225456, 0.83560002)\n",
      "(374, 0.87672728, 0.89579999)\n",
      "(375, 0.88501817, 0.90079999)\n",
      "(376, 0.88290912, 0.89569998)\n",
      "(377, 0.84872729, 0.86790001)\n",
      "(378, 0.84625453, 0.86430001)\n",
      "(379, 0.82749093, 0.8527)\n",
      "(380, 0.90563637, 0.9188)\n",
      "(381, 0.76603639, 0.80860001)\n",
      "(382, 0.82363635, 0.84890002)\n",
      "(383, 0.88801819, 0.90109998)\n",
      "(384, 0.86087275, 0.87840003)\n",
      "(385, 0.9023273, 0.91900003)\n",
      "(386, 0.90801817, 0.91909999)\n",
      "(387, 0.80790907, 0.82880002)\n",
      "(388, 0.83472729, 0.85589999)\n",
      "(389, 0.88225454, 0.90170002)\n",
      "(390, 0.74003637, 0.773)\n",
      "(391, 0.83550906, 0.86150002)\n",
      "(392, 0.9418, 0.94929999)\n",
      "(393, 0.79299998, 0.82920003)\n",
      "(394, 0.81610906, 0.85149997)\n",
      "(395, 0.7767818, 0.81559998)\n",
      "(396, 0.83152729, 0.86009997)\n",
      "(397, 0.8276909, 0.85290003)\n",
      "(398, 0.89496362, 0.91219997)\n",
      "(399, 0.88612729, 0.9077)\n",
      "(400, 0.78712726, 0.8272)\n",
      "(401, 0.90267271, 0.91769999)\n",
      "(402, 0.76856363, 0.81080002)\n",
      "(403, 0.926, 0.93849999)\n",
      "(404, 0.79983634, 0.82950002)\n",
      "(405, 0.91396362, 0.92970002)\n",
      "(406, 0.83509094, 0.8524)\n",
      "(407, 0.85792726, 0.87879997)\n",
      "(408, 0.78916365, 0.81639999)\n",
      "(409, 0.82667273, 0.85219997)\n",
      "(410, 0.73720002, 0.76450002)\n",
      "(411, 0.83380002, 0.85549998)\n",
      "(412, 0.79629093, 0.82999998)\n",
      "(413, 0.97810906, 0.97930002)\n",
      "(414, 0.87083638, 0.89300001)\n",
      "(415, 0.8046909, 0.82789999)\n",
      "(416, 0.9580909, 0.9601)\n",
      "(417, 0.87525457, 0.89310002)\n",
      "(418, 0.88876367, 0.9034)\n",
      "(419, 0.80752724, 0.84109998)\n",
      "(420, 0.88541818, 0.9012)\n",
      "(421, 0.8555091, 0.88160002)\n",
      "(422, 0.89383638, 0.91039997)\n",
      "(423, 0.96861815, 0.97109997)\n",
      "(424, 0.87583637, 0.89349997)\n",
      "(425, 0.80952728, 0.82870001)\n",
      "(426, 0.90167272, 0.91649997)\n",
      "(427, 0.91143638, 0.92629999)\n",
      "(428, 0.71016365, 0.7579)\n",
      "(429, 0.89001817, 0.90869999)\n",
      "(430, 0.83954543, 0.86809999)\n",
      "(431, 0.80354548, 0.82309997)\n",
      "(432, 0.85839999, 0.87150002)\n",
      "(433, 0.84785455, 0.87400001)\n",
      "(434, 0.94687271, 0.95679998)\n",
      "(435, 0.95356363, 0.95859998)\n",
      "(436, 0.88445455, 0.90240002)\n",
      "(437, 0.91152728, 0.92079997)\n",
      "(438, 0.86623639, 0.88499999)\n",
      "(439, 0.85965455, 0.88270003)\n",
      "(440, 0.83920002, 0.86210001)\n",
      "(441, 0.78812724, 0.81339997)\n",
      "(442, 0.89647275, 0.91299999)\n",
      "(443, 0.81389093, 0.83520001)\n",
      "(444, 0.87476361, 0.89249998)\n",
      "(445, 0.76736361, 0.79180002)\n",
      "(446, 0.72505456, 0.73150003)\n",
      "(447, 0.80598181, 0.83039999)\n",
      "(448, 0.83216363, 0.84869999)\n",
      "(449, 0.77614546, 0.80379999)\n",
      "(450, 0.77614546, 0.79879999)\n",
      "(451, 0.81152725, 0.8211)\n",
      "(452, 0.76794547, 0.78289998)\n",
      "(453, 0.95941818, 0.96329999)\n",
      "(454, 0.84078181, 0.8599)\n",
      "(455, 0.80932724, 0.82910001)\n",
      "(456, 0.81872725, 0.83890003)\n",
      "(457, 0.81523639, 0.84030002)\n",
      "(458, 0.75965452, 0.78100002)\n",
      "(459, 0.80061817, 0.83020002)\n",
      "(460, 0.8123818, 0.82700002)\n",
      "(461, 0.85372728, 0.87300003)\n",
      "(462, 0.84252727, 0.85530001)\n",
      "(463, 0.83949089, 0.85579997)\n",
      "(464, 0.78614545, 0.8154)\n",
      "(465, 0.92009091, 0.93409997)\n",
      "(466, 0.82156366, 0.85229999)\n",
      "(467, 0.93356365, 0.9454)\n",
      "(468, 0.88518184, 0.90020001)\n",
      "(469, 0.88143635, 0.89219999)\n",
      "(470, 0.81185454, 0.8369)\n",
      "(471, 0.93816364, 0.94590002)\n",
      "(472, 0.86781818, 0.8858)\n",
      "(473, 0.75101817, 0.7687)\n",
      "(474, 0.7803818, 0.81099999)\n",
      "(475, 0.89205456, 0.90219998)\n",
      "(476, 0.78354543, 0.80580002)\n",
      "(477, 0.6499818, 0.66610003)\n",
      "(478, 0.74265456, 0.75150001)\n",
      "(479, 0.7949273, 0.81639999)\n",
      "(480, 0.82941818, 0.84890002)\n",
      "(481, 0.86730909, 0.88550001)\n",
      "(482, 0.69898182, 0.7119)\n",
      "(483, 0.81065452, 0.82260001)\n",
      "(484, 0.80680001, 0.82609999)\n",
      "(485, 0.82770908, 0.83759999)\n",
      "(486, 0.66159999, 0.67570001)\n",
      "(487, 0.79450911, 0.81950003)\n",
      "(488, 0.8154, 0.838)\n",
      "(489, 0.73720002, 0.76010001)\n",
      "(490, 0.71392727, 0.74110001)\n",
      "(491, 0.92570907, 0.9339)\n",
      "(492, 0.76258183, 0.78689998)\n",
      "(493, 0.70630908, 0.73790002)\n",
      "(494, 0.90025455, 0.91820002)\n",
      "(495, 0.65460002, 0.67379999)\n",
      "(496, 0.80814546, 0.83469999)\n",
      "(497, 0.89587271, 0.91289997)\n",
      "(498, 0.78601819, 0.80180001)\n",
      "(499, 0.8956182, 0.91399997)\n",
      "(500, 0.78432727, 0.79830003)\n",
      "(501, 0.79698181, 0.83160001)\n",
      "(502, 0.97067273, 0.97409999)\n",
      "(503, 0.81440002, 0.83579999)\n",
      "(504, 0.76569092, 0.79710001)\n",
      "(505, 0.86810911, 0.89139998)\n",
      "(506, 0.79936361, 0.8089)\n",
      "(507, 0.67205453, 0.70480001)\n",
      "(508, 0.67909092, 0.69199997)\n",
      "(509, 0.69114548, 0.71820003)\n",
      "(510, 0.7468909, 0.77829999)\n",
      "(511, 0.7420727, 0.75889999)\n",
      "(512, 0.63247275, 0.66509998)\n",
      "(513, 0.63279998, 0.65890002)\n",
      "(514, 0.7148909, 0.72280002)\n",
      "(515, 0.66083634, 0.68150002)\n",
      "(516, 0.78381819, 0.80440003)\n",
      "(517, 0.5902909, 0.61690003)\n",
      "(518, 0.7967273, 0.82319999)\n",
      "(519, 0.75120002, 0.77100003)\n",
      "(520, 0.9024182, 0.9163)\n",
      "(521, 0.82752728, 0.84210002)\n",
      "(522, 0.95065457, 0.95899999)\n",
      "(523, 0.71960002, 0.75089997)\n",
      "(524, 0.75141817, 0.77509999)\n",
      "(525, 0.66659999, 0.69059998)\n",
      "(526, 0.89532727, 0.90630001)\n",
      "(527, 0.73578179, 0.75910002)\n",
      "(528, 0.58312726, 0.6045)\n",
      "(529, 0.84572726, 0.86390001)\n",
      "(530, 0.56783634, 0.58329999)\n",
      "(531, 0.61925453, 0.62489998)\n",
      "(532, 0.71985453, 0.7306)\n",
      "(533, 0.95338184, 0.9601)\n",
      "(534, 0.54009092, 0.57810003)\n",
      "(535, 0.72385454, 0.74559999)\n",
      "(536, 0.71318179, 0.73890001)\n",
      "(537, 0.61570907, 0.63730001)\n",
      "(538, 0.83594543, 0.84909999)\n",
      "(539, 0.69052726, 0.69749999)\n",
      "(540, 0.73701817, 0.7543)\n",
      "(541, 0.77240002, 0.7863)\n",
      "(542, 0.66014546, 0.6832)\n",
      "(543, 0.63829088, 0.6498)\n",
      "(544, 0.76410908, 0.76849997)\n",
      "(545, 0.88952726, 0.90490001)\n",
      "(546, 0.72225457, 0.7432)\n",
      "(547, 0.66110909, 0.66579998)\n",
      "(548, 0.74199998, 0.75220001)\n",
      "(549, 0.81352729, 0.8344)\n",
      "(550, 0.63738179, 0.6541)\n",
      "(551, 0.71072727, 0.73729998)\n",
      "(552, 0.60954547, 0.6311)\n",
      "(553, 0.95661819, 0.96210003)\n",
      "(554, 0.62581819, 0.63639998)\n",
      "(555, 0.70405453, 0.71340001)\n",
      "(556, 0.6892727, 0.69700003)\n",
      "(557, 0.67252725, 0.6807)\n",
      "(558, 0.73187274, 0.75650001)\n",
      "(559, 0.77341819, 0.78680003)\n",
      "(560, 0.66790909, 0.6832)\n",
      "(561, 0.78316361, 0.79269999)\n",
      "(562, 0.66259998, 0.67000002)\n",
      "(563, 0.72558182, 0.73280001)\n",
      "(564, 0.53427273, 0.54890001)\n",
      "(565, 0.69512725, 0.70569998)\n",
      "(566, 0.50220001, 0.52160001)\n",
      "(567, 0.74738181, 0.77410001)\n",
      "(568, 0.73325455, 0.74900001)\n",
      "(569, 0.73860002, 0.75480002)\n",
      "(570, 0.61638182, 0.62129998)\n",
      "(571, 0.59774548, 0.59759998)\n",
      "(572, 0.79723638, 0.82010001)\n",
      "(573, 0.73641819, 0.74900001)\n",
      "(574, 0.87639999, 0.88489997)\n",
      "(575, 0.51209092, 0.5302)\n",
      "(576, 0.66718179, 0.68339998)\n",
      "(577, 0.6214, 0.63279998)\n",
      "(578, 0.79043639, 0.80290002)\n",
      "(579, 0.75527275, 0.77929997)\n",
      "(580, 0.77227271, 0.78750002)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5e9effbbaa2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mteX_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mteY_0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 340\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    341\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 564\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 637\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    638\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    642\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m--> 628\u001b[0;31m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_accuracies = []\n",
    "train_accuracies = []\n",
    "with tf.Session() as sess:\n",
    "    # create a log writer. run 'tensorboard --logdir=./logs/{logfile}'\n",
    "    writer = tf.train.SummaryWriter(\"./logs/{0}\".format(logfile), sess.graph) # for 0.8\n",
    "    merged = tf.merge_all_summaries()\n",
    " \n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        infiminst.next_epoch()\n",
    "        mnist = input_data.read_data_sets(\".\", one_hot=True)\n",
    "    \n",
    "        trX, trY = mnist.train.images, mnist.train.labels\n",
    "        teX, teY = mnist.test.images, mnist.test.labels\n",
    "        \n",
    "        for start, end in zip(range(0, len(trX), batch_size), range(batch_size, len(trX), batch_size)):\n",
    "            sess.run(train_op, feed_dict={x: trX[start:end], y: trY[start:end]})\n",
    "                 \n",
    "        summary, trn_acc, tst_acc = sess.run([merged, train_acc_op, test_acc_op], feed_dict={x: trX, y: trY, xt: teX_0, yt: teY_0})\n",
    "        writer.add_summary(summary, epoch)  \n",
    "        \n",
    "        print(epoch, trn_acc, tst_acc)\n",
    "        train_accuracies.append(trn_acc)\n",
    "        test_accuracies.append(tst_acc)\n",
    "\n",
    "        writer.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
